{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da09923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import zipfile\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "pattern = re.compile('.*-(MONEY|QUANTITY|PERCENT|ORDINAL|DATE|TIME|CARDINAL)|(Other)')\n",
    "pattern_symbols = re.compile('^[\\.\\\\\\/\\[\\]\\(\\),\\-\\'\\\"\\?\\!\\“\\”\\’@:;–]+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df34522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c432c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return [nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964941d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words = stop_words.union({\"reuters\", \"bbc\"})\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and len(token)>3]\n",
    "    filtered_tokens = [token for token in filtered_tokens if not pattern_symbols.match(token.lower())]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ec8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file, filename, texts):\n",
    "    content = file.read(filename)\n",
    "    if type(content) == bytes:\n",
    "        text = content.decode('utf-8')\n",
    "        texts.append(text)\n",
    "\n",
    "    if len(content.strip()) == 0:\n",
    "        print(\"No text was found\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a157afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, word2idx, max_len):\n",
    "    # Split the text into tokens\n",
    "    sentences = split_text(text)\n",
    "\n",
    "    # Convert the tokens to integer IDs using the word2id dictionary\n",
    "    ids = []\n",
    "    endpad_idx = word2idx['ENDPAD']\n",
    "    for tokens in sentences:\n",
    "        array = []\n",
    "        for token in tokens:\n",
    "            if token in word2idx.keys():\n",
    "                array.append(word2idx[token])\n",
    "            else:\n",
    "                array.append(0)\n",
    "\n",
    "        while len(array) < max_len:\n",
    "            array.append(endpad_idx)\n",
    "        ids.append(array)\n",
    "\n",
    "    return ids, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb2fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_2(text, model, tags):\n",
    "    ents = []\n",
    "    labels = []\n",
    "    for i in range(len(text)):\n",
    "        p = model.predict(np.array([text[0][i]]), verbose=0)\n",
    "        p = np.argmax(p, axis=-1)\n",
    "        for idx, pred in enumerate(p[0][0:len(text[1][i])]):\n",
    "            if len(word := text[1][i][idx]) > 3:\n",
    "                ents.append(word.lower())\n",
    "                if not pattern.match(tags[pred]):\n",
    "                    # append the same word once more to increase its statistics and weight artificially\n",
    "                    ents.append(word.lower())\n",
    "#                     ents.append(word.lower())\n",
    "                    labels.append(tags[pred])\n",
    "    ents = remove_stopwords(ents)\n",
    "    return ents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b9199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def get_labeled_words(model, texts, word2idx):\n",
    "    proc_labels = []\n",
    "    proc_ents = []\n",
    "    for text in texts:\n",
    "        processed = preprocess_text(text, word2idx, model.layers[0].output_shape[0][1])\n",
    "        ents, labels = predict_text_2(processed, model, tags)\n",
    "        proc_ents.append(remove_stopwords(ents))\n",
    "        proc_labels.append(labels)\n",
    "    lemmatized_text = [lemmatize_tokens(doc) for doc in proc_ents]\n",
    "    return lemmatized_text, proc_labels\n",
    "\n",
    "def get_labeled_words_2(texts):\n",
    "    proc_labels = []\n",
    "    proc_ents = []\n",
    "    for text in texts:\n",
    "        processed = split_text(text)\n",
    "        ents = [word for sentence in processed for word in sentence]\n",
    "\n",
    "        proc_ents.append(remove_stopwords(ents))\n",
    "    lemmatized_text = [lemmatize_tokens(doc) for doc in proc_ents]\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "# with zipfile.ZipFile(\"data/articles_2021-11-05_1000.zip\", \"r\") as f:\n",
    "# with zipfile.ZipFile(\"data/articles_2023-01-07_2000.zip\", \"r\") as f:\n",
    "# with zipfile.ZipFile(\"data/articles_2023-02-04_500.zip\", \"r\") as f:\n",
    "with zipfile.ZipFile(\"data/articles_2023-02-09_1000.zip\", \"r\") as f:\n",
    "    total_f = len(f.namelist())\n",
    "    counter = 1\n",
    "    for filename in f.namelist():\n",
    "        counter += 1\n",
    "        process_file(f, filename, texts)\n",
    "    f.close()\n",
    "\n",
    "model = load_model('models/my_model2/model.h5')\n",
    "# ----------------------------#\n",
    "with open('models/my_model2/tags.pickle', 'rb') as handle:\n",
    "    tags = pickle.load(handle)\n",
    "\n",
    "with open('models/my_model2/words.pickle', 'rb') as handle:\n",
    "    word2idx = pickle.load(handle)\n",
    "\n",
    "\n",
    "proc_ents, proc_labels = get_labeled_words(model, texts, word2idx)\n",
    "\n",
    "# proc_ents = get_labeled_words_2(texts)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95354b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping named entities to integer ids\n",
    "dictionary = Dictionary(proc_ents)\n",
    "\n",
    "# Create a document-term matrix where each document is a text and each term is a named entity\n",
    "corpus = [dictionary.doc2bow(text) for text in proc_ents]\n",
    "\n",
    "# Train the LDA model on the corpus\n",
    "#lda_model = LdaModel(corpus, num_topics=6)\n",
    "\n",
    "num_topics = 6\n",
    "\n",
    "# lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "#                                         id2word=dictionary,\n",
    "#                                         num_topics=num_topics,\n",
    "#                                         random_state=100,\n",
    "#                                         workers=7,\n",
    "#                                         chunksize=10,\n",
    "#                                         passes=20,\n",
    "#                                         alpha='symmetric',\n",
    "#                                         iterations=200,\n",
    "#                                         per_word_topics=False)\n",
    "\n",
    "# # Infer the topic distribution for each text\n",
    "# # text_topics = [lda_model[c] for c in corpus]\n",
    "\n",
    "# for i in range(num_topics):\n",
    "#     topic_words = lda_model.show_topic(i)\n",
    "#     tmp = [(word, prob) for word, prob in topic_words]\n",
    "#     print(\"Topic %d: \\n%s\\n\" % (i, tmp))\n",
    "\n",
    "# # print(text_topics[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(iteration, total):\n",
    "    total_len = 100\n",
    "    percent_part = (\"{0:.2f}\").format(100 * (iteration / total))\n",
    "    filled = int(total_len * iteration / total)\n",
    "    bar = '█' * filled + '-' * (total_len - filled)\n",
    "    print(f'\\r Progress: [{bar}] {percent_part}%', end='')\n",
    "    if iteration == total:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_grid_search(texts, corpus, id2word, hyperparams_list, coherence_cv_scores, coherence_umass_scores, perplexity_scores, dir_path, with_training):\n",
    "    counter = 1\n",
    "\n",
    "    if with_training:\n",
    "        for elem in hyperparams_list:\n",
    "            num_topics, alpha = elem.values()\n",
    "            progress_bar(counter, len(hyperparams_list))\n",
    "            counter += 1\n",
    "            lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                        id2word=id2word,\n",
    "                                                        num_topics=num_topics,\n",
    "                                                        workers=19,\n",
    "                                                        random_state=100,\n",
    "                                                        chunksize=100,\n",
    "                                                        passes=10,\n",
    "                                                        alpha=alpha,\n",
    "                                                        iterations=200,\n",
    "                                                        per_word_topics=False)\n",
    "\n",
    "            joblib.dump(lda_model, dir_path + str(num_topics) + 'topics_' + alpha + '_new.jl')\n",
    "\n",
    "    start_time = time.time()\n",
    "    counter = 1\n",
    "    for elem in hyperparams_list:\n",
    "        num_topics, alpha = elem.values()\n",
    "        progress_bar(counter, len(hyperparams_list))\n",
    "        counter += 1\n",
    "\n",
    "        lda_model = joblib.load(dir_path + str(num_topics) + 'topics_' + alpha + '_new.jl')\n",
    "\n",
    "        # Coherence model to get coherence score, based on currently used corpus and dictionary\n",
    "        coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=texts,\n",
    "                                                           dictionary=id2word, coherence='c_v')\n",
    "\n",
    "        coherence_cv_scores.append({\"num_topics\": num_topics,\n",
    "                                 \"C_V coherence\": coherence_model_lda.get_coherence()})\n",
    "\n",
    "        # Coherence model to get coherence score, based on currently used corpus and dictionary\n",
    "        coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "\n",
    "        coherence_umass_scores.append({\"num_topics\": num_topics,\n",
    "                                 \"U_Mass coherence\": coherence_model_lda.get_coherence()})\n",
    "\n",
    "        perplexity_scores.append({\"num_topics\": num_topics,\n",
    "                                  \"perplexity\": lda_model.log_perplexity(corpus)})\n",
    "\n",
    "    final_time = time.time() - start_time\n",
    "    print(\"Total time spent: \" + str(final_time) + \" seconds\")\n",
    "\n",
    "    alpha = hyperparams_list[0][\"alpha\"]\n",
    "#     joblib.dump(coherence_cv_scores, dir_path + \"coherence_cv_scores_\" + alpha + \"_alpha.jl\")\n",
    "#     joblib.dump(coherence_umass_scores, dir_path + \"coherence_umass_scores_\" + alpha + \"_alpha.jl\")\n",
    "#     joblib.dump(perplexity_scores, dir_path + \"perplexity_scores_\" + alpha + \"_alpha.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aace20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import time\n",
    "\n",
    "hyperparams_list = [{\"num_topics\": 4, \"alpha\": \"symmetric\"},\n",
    "                    {\"num_topics\": 6, \"alpha\": \"symmetric\"},\n",
    "                    {\"num_topics\": 8, \"alpha\": \"symmetric\"},\n",
    "                    {\"num_topics\": 10, \"alpha\": \"symmetric\"},\n",
    "                    {\"num_topics\": 12, \"alpha\": \"symmetric\"},\n",
    "                    {\"num_topics\": 14, \"alpha\": \"symmetric\"}]\n",
    "\n",
    "hyperparams_list_2 = [{\"num_topics\": 4, \"alpha\": \"asymmetric\"},\n",
    "                      {\"num_topics\": 6, \"alpha\": \"asymmetric\"},\n",
    "                      {\"num_topics\": 8, \"alpha\": \"asymmetric\"},\n",
    "                      {\"num_topics\": 10, \"alpha\": \"asymmetric\"},\n",
    "                      {\"num_topics\": 12, \"alpha\": \"asymmetric\"},\n",
    "                      {\"num_topics\": 14, \"alpha\": \"asymmetric\"}]\n",
    "\n",
    "coherence_cv_scores = []\n",
    "coherence_umass_scores = []\n",
    "perplexity_scores = []\n",
    "dir_path = \"data/hyperparameter_tuning/\"\n",
    "\n",
    "custom_grid_search(proc_ents, corpus, dictionary, hyperparams_list,\n",
    "                   coherence_cv_scores, coherence_umass_scores, perplexity_scores, dir_path, True)\n",
    "\n",
    "# custom_grid_search(proc_ents, corpus, dictionary, hyperparams_list_2,\n",
    "#                    coherence_cv_scores, coherence_umass_scores, perplexity_scores, dir_path, True)\n",
    "\n",
    "print(\"Coherence_cv:\\n\" + str(coherence_cv_scores))\n",
    "print(\"Coherence_umass:\\n\" + str(coherence_umass_scores))\n",
    "print(\"Perplexity:\\n\" + str(perplexity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ea50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import LdaModel\n",
    "# from gensim.models import CoherenceModel\n",
    "# import itertools\n",
    "\n",
    "# # Define the range of hyperparameters to search over\n",
    "# num_topics = [4, 6, 8, 10]\n",
    "# chunksize = [100, 200, 300]\n",
    "# passes = [10, 20, 30, 40]\n",
    "\n",
    "# # Create a list of all possible hyperparameter combinations\n",
    "# hyperparameters = list(itertools.product(num_topics, chunksize, passes))\n",
    "\n",
    "# # Initialize variables to store the best model and coherence score\n",
    "# best_model = None\n",
    "# best_coherence_score = -float('inf')\n",
    "\n",
    "# counter = 0\n",
    "# progress_bar(counter, len(hyperparameters))\n",
    "\n",
    "# # Loop through each hyperparameter combination and train an LDA model\n",
    "# for params in hyperparameters:\n",
    "    \n",
    "#     num_topics, chunksize, passes = params\n",
    "#     lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "#                          id2word=dictionary,\n",
    "#                          workers=19,\n",
    "#                          num_topics=num_topics,\n",
    "#                          chunksize=chunksize,\n",
    "#                          passes=passes,\n",
    "#                          alpha='symmetric',\n",
    "#                          per_word_topics=False)\n",
    "    \n",
    "#     # Compute the coherence score for the current model\n",
    "#     coherence_model = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "#     coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "#     # If the coherence score is better than the best seen so far, update the best model and score\n",
    "#     if coherence_score > best_coherence_score:\n",
    "#         best_model = lda_model\n",
    "#         best_coherence_score = coherence_score\n",
    "    \n",
    "#     counter += 1\n",
    "#     progress_bar(counter, len(hyperparameters))\n",
    "        \n",
    "# joblib.dump(lda_model, \"data/hyperparameter_tuning/\" + str(best_model.num_topics) + 'topics.jl')\n",
    "        \n",
    "# # Print the best hyperparameters and coherence score\n",
    "# print(\"Best hyperparameters: num_topics={}, chunksize={}, passes={}\".format(best_model.num_topics, best_model.chunksize, best_model.passes))\n",
    "# print(\"Best coherence score: {}\".format(best_coherence_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619462ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_results(data):\n",
    "    keys = list(data[0].keys())\n",
    "    x = [d[keys[0]] for d in data]\n",
    "    y = [d[keys[1]] for d in data]\n",
    "\n",
    "    plt.bar(x, y, align='center')\n",
    "    plt.xlabel('Number of Topics')\n",
    "    plt.ylabel(keys[1])\n",
    "    plt.title('Histogram of {}'.format(keys[1]))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(coherence_cv_scores)\n",
    "visualize_results(coherence_umass_scores)\n",
    "visualize_results(perplexity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = joblib.load(dir_path + str(4) + 'topics_symmetric_new.jl')\n",
    "for i in range(4):\n",
    "    topic_words = lda_model.show_topic(i)\n",
    "    tmp = [(word, prob) for word, prob in topic_words]\n",
    "    print(\"Topic %d: \\n%s\\n\" % (i, tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# Prepare the data for visualization\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Show the visualization\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a948774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "def t_SNE_clustering(lda_model, corpus):\n",
    "    if (len(corpus) < 2):\n",
    "        print(\"Required 2 or more texts in the corpus\")\n",
    "        return\n",
    "    topic_weights = []\n",
    "    for i, row_list in enumerate(lda_model[corpus]):\n",
    "        row = row_list[0] if lda_model.per_word_topics else row_list\n",
    "        topic_weights.append([w for i, w in row])\n",
    "\n",
    "\n",
    "    # Array of topic weights\n",
    "    arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "    # Keep the well separated points (optional)\n",
    "    arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "    # Dominant topic number in each doc\n",
    "    topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "    print(\"Choose perplexity (neigbors=3*perplexity):\")\n",
    "    perpl = int(input())\n",
    "\n",
    "    if perpl < 0:\n",
    "        perpl = 30\n",
    "\n",
    "    # tSNE Dimension Reduction\n",
    "    tsne_model = TSNE(n_components=3, verbose=1, random_state=0, angle=.99, init='pca', n_jobs=7, perplexity=perpl)\n",
    "    tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "    # Plot the Topic Clusters using Bokeh\n",
    "    n_topics = lda_model.num_topics\n",
    "    mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1], color=mycolors[topic_num])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfca85e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_SNE_clustering(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95643110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
